{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 Machine Learning\n",
    "\n",
    "### Authors:\n",
    "Alfredo Ibias Martínez  \n",
    "Miguel Benito Parejo\n",
    "\n",
    "The aim of this Lab is to preprocess and audit a given dataset. In order to do so, we analyse the data to prepare it for future uses on machine learning tasks.  \n",
    "The given dataset has 32561 entries with the following variables:  \n",
    "- age\n",
    "- workclass\n",
    "- fnlwgt\n",
    "- education\n",
    "- education-num\n",
    "- marital-status\n",
    "- ocupation\n",
    "- relationship\n",
    "- race\n",
    "- sex\n",
    "- capital-gain\n",
    "- capital-loss\n",
    "- hours-per-week\n",
    "- native-country\n",
    "\n",
    "And they are classified in the following two classes:\n",
    "- <=50K\n",
    "- \\>50K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to import the libraries we will use along the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "#import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can upload our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "\n",
    "dataset = pd.read_csv('./Adult_Income.txt',delimiter=',',header=0)\n",
    "#dataset\n",
    "\n",
    "# Header\n",
    "header = []\n",
    "for row in dataset:\n",
    "    header.append(row)\n",
    "#header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have upload our dataset, we face our first problem. It was what to do with unknown data.  \n",
    "As our database is large, we first decided to check how many entries lack some data. Therefore, we delete these entries from our dataset and check how many entries we still have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1  \n",
    "First, lets see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                age        fnlwgt   education-num   capital-gain  \\\n",
      "count  32561.000000  3.256100e+04    32561.000000   32561.000000   \n",
      "mean      38.581647  1.897784e+05       10.080679    1077.648844   \n",
      "std       13.640433  1.055500e+05        2.572720    7385.292085   \n",
      "min       17.000000  1.228500e+04        1.000000       0.000000   \n",
      "25%       28.000000  1.178270e+05        9.000000       0.000000   \n",
      "50%       37.000000  1.783560e+05       10.000000       0.000000   \n",
      "75%       48.000000  2.370510e+05       12.000000       0.000000   \n",
      "max       90.000000  1.484705e+06       16.000000   99999.000000   \n",
      "\n",
      "        capital-loss   hours-per-week         class  \n",
      "count   32561.000000     32561.000000  32561.000000  \n",
      "mean       87.303830        40.437456      0.240810  \n",
      "std       402.960219        12.347429      0.427581  \n",
      "min         0.000000         1.000000      0.000000  \n",
      "25%         0.000000        40.000000      0.000000  \n",
      "50%         0.000000        40.000000      0.000000  \n",
      "75%         0.000000        45.000000      0.000000  \n",
      "max      4356.000000        99.000000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "for column in dataset.columns:\n",
    "    count = 0\n",
    "    for elem in dataset[column]:\n",
    "        if elem == \" ?\":\n",
    "            dataset = dataset.drop(count)\n",
    "            pos.append(count)\n",
    "        count += 1\n",
    "        while pos.__contains__(count):\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                age        fnlwgt   education-num   capital-gain  \\\n",
      "count  30162.000000  3.016200e+04    30162.000000   30162.000000   \n",
      "mean      38.437902  1.897938e+05       10.121312    1092.007858   \n",
      "std       13.134665  1.056530e+05        2.549995    7406.346497   \n",
      "min       17.000000  1.376900e+04        1.000000       0.000000   \n",
      "25%       28.000000  1.176272e+05        9.000000       0.000000   \n",
      "50%       37.000000  1.784250e+05       10.000000       0.000000   \n",
      "75%       47.000000  2.376285e+05       13.000000       0.000000   \n",
      "max       90.000000  1.484705e+06       16.000000   99999.000000   \n",
      "\n",
      "        capital-loss   hours-per-week         class  \n",
      "count   30162.000000     30162.000000  30162.000000  \n",
      "mean       88.372489        40.931238      0.248922  \n",
      "std       404.298370        11.979984      0.432396  \n",
      "min         0.000000         1.000000      0.000000  \n",
      "25%         0.000000        40.000000      0.000000  \n",
      "50%         0.000000        40.000000      0.000000  \n",
      "75%         0.000000        45.000000      0.000000  \n",
      "max      4356.000000        99.000000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, before deleting any entry we had 32561 entries, and now we left 30162 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2399"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial = 32561\n",
    "final = 30162\n",
    "deleted = initial-final\n",
    "deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07367709836921471"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage = deleted/initial\n",
    "percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we have deleted 2399 entries, that are only a 7.37% of the total entries. Thus, we can assume that loss. Also, we have analysed the distribution of deleted entries and he have concluded that they are not significative enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets focus on the education and education-num variables. Doing a quick analysis we have shown that they are exactly the same variable, but expressed in a different data type, so we decided to delete the education column. Also, according to our criteria and most of the information we found on the Internet, we found that the column fnlwgt is an almost random number. Therefore, we decided to delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(\" fnlwgt\", axis=1)\n",
    "dataset = dataset.drop(\" education\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have deleted the extra or incomplete information, we can focus on nominal variables. In our case, we decided to transform them into a set of dummy variables, so we can work with them properly, in the sense that we don't give any one possible value more relevance than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last transformation we will do to our dataset will be the detection and management of outliers. In order to do so, first, we need to compute the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr = dataset.quantile(q=0.75, axis=0)-dataset.quantile(q=0.25, axis=0)\n",
    "q1 = dataset.quantile(q=0.25, axis=0) - 1.5*iqr\n",
    "q3 = dataset.quantile(q=0.75, axis=0) + 1.5*iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along all the possible outliers, we decided that the only ones that we think we can delete are the ones from the variable education-num, because we think the others are relevant enough to keep them as they follow a wider distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "poss = []\n",
    "for elem in dataset[header[4]] < q1[\" education-num\"]: #They are 196\n",
    "    if elem:\n",
    "        poss.append(i)\n",
    "    i += 1\n",
    "    while pos.__contains__(i):\n",
    "        i += 1\n",
    "\n",
    "for i in poss:\n",
    "    dataset = dataset.drop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have our database ready for work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets discuss the points from the instructions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción de las variables y valores estadísticos (minimo, máximo, media, desviación,mediana, etc.). Estudia qué valores estadísticos son los convenientes según el tipo de variable y procede en consecuencia.\n",
    "\n",
    "The variable types are:\n",
    "- age (numerical)\n",
    "- workclass (nominal) --> To dummy variables\n",
    "- fnlwgt (numerical) --> Deleted\n",
    "- education (nominal) --> Deleted\n",
    "- education-num (numerical)\n",
    "- marital-status  (nominal) --> To dummy variables\n",
    "- ocupation  (nominal) --> To dummy variables\n",
    "- relationship  (nominal) --> To dummy variables\n",
    "- race (nominal) --> To dummy variables\n",
    "- sex (nominal) --> To dummy variables\n",
    "- capital-gain (numerical)\n",
    "- capital-loss (numerical)\n",
    "- hours-per-week (numerical)\n",
    "- native-country (nominal) --> To dummy variables\n",
    "\n",
    "The statistical values have been shown at the beginning, at 1. As we can see, there's no statistical values for nominal variables, as they cannot be numerically analysed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe y realiza modificaciones en la base datos si lo consideras necesario. Por ejemplo, qué harías con valores nominales, si los hubiera.\n",
    "\n",
    "We have transformed the database previously. We did it by:\n",
    "- Deleting entries with missing values.\n",
    "- Deleting the columns of education and fnlwgt.\n",
    "- Transforming nominal variables into dummy variables.\n",
    "- Detecting and treating outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudia si es necesario normalizar los datos y cómo lo harías. Procede a modificar la base de datos (normalizar) si lo consideras necesario.\n",
    "\n",
    "We analysed the database and concluded that we don't need to normalise any variable, as it wouldn't give us any extra information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de valores extremos (outliers) y descripción de qué harías en cada caso.\n",
    "\n",
    "We have detected them and decided to act as follows:\n",
    "- Delete the outliers of the variable education-num.\n",
    "- Keep the outliers of the variables age and hours-per-week because they follow a wider distribution.\n",
    "- Keep the outliers of the variables capital-gain and capital-loss because they would provide no information otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de valores perdidos (missing values) y descripción de cómo actuarías para solventar el problema.\n",
    "\n",
    "We detected them and we decided that they are few enogh to keep an useful database after deleting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar correlaciones entre:\n",
    "##### las variables predictoras, lo que permitirá ver si hay variables redundantes.\n",
    "We only found a high correlation between the variables education and education-num, and therefore we deleted the variable education. Also, we found a high correlation between the variables marital-status_ Married-civ-spouse and relationship_ Husband, of 0.9, but as they are dummy variables, we still keep them.\n",
    "##### variables predictoras y la clase (target).\n",
    "We found no correlation between the variables and the target class enough to claim that they are a predictive variable. Anyway, we found a high correlation between the class and the variable marital-status_ Married-civ-spouse, of 0.45, but not enough to assert that it is a predictive variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecta, si hubiera, falsos predictores.\n",
    "As we have no variable with a high enough correlation with the target class, we have no false predictive variable to be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudia si fuera conveniente segmentar alguna de las variables.\n",
    "We analysed the variables and determined that the only variable that could be segmentated is the age variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudia si fuera conveniente crear nuevas variables sintéticas basada en las variables originales.\n",
    "As we have nominal variables, we determined that it is convenient to create the needed dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We used Python3.6 and Spyder in order to see and analyse the data for our work, but these values are not shown in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
